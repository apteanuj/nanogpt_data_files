{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfc730a-249c-455a-9d95-b279874bcfb1",
   "metadata": {},
   "source": [
    "We will use this notebook to check timing and precision of various \n",
    "algorithms to compute the orthogonalized version of a matrix. \n",
    "It is important to nail down the appropriate matrix sizes and their rough \n",
    "entry structure (based on initialization) so that we can be sure that gains here will translate into faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85521dc8-265c-4043-879b-3e7de7e1ed49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T18:14:20.072599Z",
     "iopub.status.busy": "2025-08-06T18:14:20.072429Z",
     "iopub.status.idle": "2025-08-06T18:14:23.426730Z",
     "shell.execute_reply": "2025-08-06T18:14:23.426251Z",
     "shell.execute_reply.started": "2025-08-06T18:14:20.072582Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626a531-e161-435e-b6f0-e3ec22673624",
   "metadata": {},
   "source": [
    "We will also study the Newton Schulz method that is in the NanoGPT speedrun. Note that it can be sped up by noticing that symmetric matrix multiplication can be done with fewer operations that GEMM. This is used by Bryon Xu who wrote a custom Triton Kernel to do this to set the current record [https://github.com/KellerJordan/modded-nanogpt/pull/109]. However, if we can improve upon the existing orthogonalization algorithm (without the triton kernel) we can then use the Triton kernel to compute G^T G more efficiently and use that in our version of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff990c69-93e1-4e1a-85b7-160ddadbc1f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T18:14:23.428591Z",
     "iopub.status.busy": "2025-08-06T18:14:23.428448Z",
     "iopub.status.idle": "2025-08-06T18:14:30.363304Z",
     "shell.execute_reply": "2025-08-06T18:14:30.362796Z",
     "shell.execute_reply.started": "2025-08-06T18:14:23.428575Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.to(torch.bfloat16)\n",
    "    # X = G\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5a9223e-48af-4169-a72b-f2deb2620a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:50:58.867361Z",
     "iopub.status.busy": "2025-08-06T21:50:58.867162Z",
     "iopub.status.idle": "2025-08-06T21:50:58.878331Z",
     "shell.execute_reply": "2025-08-06T21:50:58.877858Z",
     "shell.execute_reply.started": "2025-08-06T21:50:58.867343Z"
    }
   },
   "outputs": [],
   "source": [
    "coeffs_list = [\n",
    "    (8.28721201814563,  -23.595886519098837, 17.300387312530933),\n",
    "    (4.107059111542203,  -2.9478499167379106,  0.5448431082926601),\n",
    "    (3.9486908534822946, -2.908902115962949,   0.5518191394370137),\n",
    "    (3.3184196573706015, -2.488488024314874,   0.51004894012372),\n",
    "    (2.300652019954817,  -1.6689039845747493,  0.4188073119525673),\n",
    "    (1.891301407787398,  -1.2679958271945868,  0.37680408948524835),\n",
    "    (1.8750014808534479, -1.2500016453999487,  0.3750001645474248),\n",
    "    (1.875,              -1.25,                 0.375)   # fixed-point tail\n",
    "]\n",
    "\n",
    "coeffs_list = [(a/1.01, b/1.01**3, c/1.01**5) for (a,b,c) in coeffs_list[:-1]] + [coeffs_list[-1]]\n",
    "\n",
    "@torch.compile\n",
    "def PolarExpress(G: torch.tensor, steps: int) -> torch.tensor:\n",
    "    assert G.ndim >= 2\n",
    "    X = G.to(torch.bfloat16)\n",
    "    if G.size(-2) > G.size(-1):          # work on the cheaper orientation\n",
    "        X = X.mT\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True)*1.01 + 1e-7)\n",
    "    hs = coeffs_list[:steps] + [coeffs_list[-1]]*max(0, steps-len(coeffs_list))\n",
    "    for a,b,c in hs:\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A\n",
    "        X = a * X + B @ X\n",
    "    if G.size(-2) > G.size(-1):          # work on the cheaper orientation\n",
    "        X = X.mT\n",
    "    return X\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def PolarExpressRect(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
    "    assert G.ndim >= 2\n",
    "    X = G.to(torch.bfloat16)\n",
    "\n",
    "    # 1. cheap orientation\n",
    "    transposed = G.size(-2) > G.size(-1)\n",
    "    if transposed:\n",
    "        X = X.mT                      # rows <= cols\n",
    "\n",
    "    # 2. scale\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True)*1.01 + 1e-7)\n",
    "\n",
    "    # 3. coefficient list\n",
    "    hs = coeffs_list[:steps] + [coeffs_list[-1]]*max(0, steps-len(coeffs_list))\n",
    "\n",
    "    # 4. Gram in row-space\n",
    "    Y  = X @ X.mT\n",
    "    lam = 1e-4\n",
    "    Y.diagonal().add_(lam)\n",
    "    I  = torch.eye(Y.size(-1), dtype=X.dtype, device=X.device)\n",
    "    Q  = I.clone()\n",
    "\n",
    "    # 5. quintic iteration\n",
    "    for a, b, c in hs:\n",
    "        R  = Q.mT @ Y @ Q\n",
    "        hR = a*I + b*R + c*(R @ R)\n",
    "        Q  = Q @ hR\n",
    "\n",
    "    # 6. apply on the left\n",
    "    X = Q @ X\n",
    "\n",
    "    # 7. restore layout\n",
    "    if transposed:\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8febca54-fe32-4737-b4dd-ce29504a3776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:50:59.203150Z",
     "iopub.status.busy": "2025-08-06T21:50:59.202961Z",
     "iopub.status.idle": "2025-08-06T21:50:59.208218Z",
     "shell.execute_reply": "2025-08-06T21:50:59.207687Z",
     "shell.execute_reply.started": "2025-08-06T21:50:59.203134Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def CholeskySVD(G: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-7\n",
    "    assert G.ndim >= 2\n",
    "    X = G.to(torch.bfloat16)\n",
    "    if G.size(-1) > G.size(-2):\n",
    "        X = X.mT\n",
    "    Gram = (X.mT @ X).to(torch.float32)\n",
    "    Gram.diagonal(dim1=-2, dim2=-1).add_(eps)\n",
    "    eigvals, V = torch.linalg.eigh(Gram)\n",
    "    inv_sqrt = torch.rsqrt(torch.clamp(eigvals, min=eps)).to(torch.bfloat16)\n",
    "    V = V.to(torch.bfloat16)\n",
    "    V_scaled = V * inv_sqrt.unsqueeze(-2)\n",
    "    U = X @ (V_scaled @ V.mT)\n",
    "    if G.size(-1) > G.size(-2):\n",
    "        U = U.mT\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9def374-232e-466e-b5bf-98d0ad7d63c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:50:59.482867Z",
     "iopub.status.busy": "2025-08-06T21:50:59.482648Z",
     "iopub.status.idle": "2025-08-06T21:50:59.488795Z",
     "shell.execute_reply": "2025-08-06T21:50:59.488326Z",
     "shell.execute_reply.started": "2025-08-06T21:50:59.482836Z"
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    "dim = 768\n",
    "#dim = 2**12\n",
    "expansion_factor = 4\n",
    "bound = math.sqrt(3) * 0.5 / math.sqrt(dim)\n",
    "\n",
    "@torch.compile\n",
    "def make_qkv_matrix():\n",
    "    G = torch.empty((dim, dim), dtype=torch.bfloat16)\n",
    "    G.uniform_(-bound, bound)\n",
    "    return G\n",
    "\n",
    "@torch.compile\n",
    "def make_mlp_matrix():\n",
    "    G = torch.empty((expansion_factor * dim, dim), dtype=torch.bfloat16)\n",
    "    G.uniform_(-bound, bound)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e1a4abf-de54-4249-91f0-59f347bafae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:51:21.300877Z",
     "iopub.status.busy": "2025-08-06T21:51:21.300656Z",
     "iopub.status.idle": "2025-08-06T21:51:29.932220Z",
     "shell.execute_reply": "2025-08-06T21:51:29.931686Z",
     "shell.execute_reply.started": "2025-08-06T21:51:21.300860Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/custom-file-systems/efs/fs-03c97ca6b7b0029ac_fsap-0d737ffa30793e96a/nanoenv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1688: FutureWarning: `torch._prims_common.check` is deprecated and will be removed in the future. Please use `torch._check*` functions instead.\n",
      "  check(\n",
      "/mnt/custom-file-systems/efs/fs-03c97ca6b7b0029ac_fsap-0d737ffa30793e96a/nanoenv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1688: FutureWarning: `torch._prims_common.check` is deprecated and will be removed in the future. Please use `torch._check*` functions instead.\n",
      "  check(\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable \n",
    "# ---------------------------------------------------------------------\n",
    "# error metric (compiled)\n",
    "# ---------------------------------------------------------------------\n",
    "@torch.compile\n",
    "def orth_error(X: torch.Tensor) -> torch.Tensor:\n",
    "    # assumes X is [..., m, n]; works batched\n",
    "    gram = X.mT @ X                        # [..., n, n]\n",
    "    eye  = torch.eye(gram.size(-1), dtype=X.dtype)\n",
    "    if gram.ndim > 2:\n",
    "        eye = eye.expand(*gram.shape[:-2], *eye.shape)\n",
    "    err = (gram - eye).pow(2).sum(dim=(-2, -1)).sqrt()\n",
    "    return err / math.sqrt(min(X.size(-2), X.size(-1)))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# timing utility (default repeats = 100)\n",
    "# ---------------------------------------------------------------------\n",
    "def time_fn(fn: Callable, *args, repeats: int = 10, **kwargs) -> float:\n",
    "    torch.cuda.empty_cache()\n",
    "    fn(*args, **kwargs)                    # warm-up compile\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        fn(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - t0) * 1000 / repeats  # ms\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# one-shot benchmark including error\n",
    "# ---------------------------------------------------------------------\n",
    "bench = []\n",
    "for name, G in [(\"qkv 768×768\", make_qkv_matrix()),\n",
    "                (\"mlp 3072×768\", make_mlp_matrix())]:\n",
    "    for alg, fn_args in [\n",
    "            (\"NS5\", (zeropower_via_newtonschulz5,   dict(steps=5))),\n",
    "            (\"PolarExpress\",     (PolarExpress,   dict(steps=5))),\n",
    "            (\"PolarExpressRect\", (PolarExpressRect, dict(steps=5))),\n",
    "            (\"CholeskySVD\",      (CholeskySVD,    {}))]:\n",
    "        fn, kw = fn_args\n",
    "        try:\n",
    "            t = time_fn(fn, G, **kw)\n",
    "            out = fn(G, **kw)\n",
    "            err = orth_error(out).item()\n",
    "            bench.append((name, alg, t, err))\n",
    "        except Exception as e:\n",
    "            # skip incompatible shape / other runtime issues\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7a012cd-88bd-4969-a995-67ff528c0599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T21:51:29.933035Z",
     "iopub.status.busy": "2025-08-06T21:51:29.932868Z",
     "iopub.status.idle": "2025-08-06T21:51:29.937432Z",
     "shell.execute_reply": "2025-08-06T21:51:29.937026Z",
     "shell.execute_reply.started": "2025-08-06T21:51:29.933020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix       | Algorithm        | Time (ms) | Error    \n",
      "-------------+------------------+-----------+----------\n",
      "qkv 768×768  | NS5              | 25.030    | 3.379e-01\n",
      "qkv 768×768  | PolarExpress     | 24.868    | 2.031e-01\n",
      "qkv 768×768  | PolarExpressRect | 35.980    | 4.023e-01\n",
      "qkv 768×768  | CholeskySVD      | 44.853    | 7.450e+01\n",
      "mlp 3072×768 | NS5              | 46.088    | 2.930e-01\n",
      "mlp 3072×768 | PolarExpress     | 46.092    | 1.758e-01\n",
      "mlp 3072×768 | PolarExpressRect | 27.897    | 2.070e-01\n",
      "mlp 3072×768 | CholeskySVD      | 48.847    | 6.653e-03\n"
     ]
    }
   ],
   "source": [
    "# assume `bench` is a list of tuples: (name, alg, time_ms (float), error (float))\n",
    "hdr = [\"Matrix\", \"Algorithm\", \"Time (ms)\", \"Error\"]\n",
    "# build rows with formatted strings\n",
    "rows = [hdr]\n",
    "for name, alg, t, err in bench:\n",
    "    rows.append([name, alg, f\"{t:.3f}\", f\"{err:.3e}\"])\n",
    "\n",
    "# compute column widths on the string rows\n",
    "col_widths = [\n",
    "    max(len(row[col]) for row in rows)\n",
    "    for col in range(len(hdr))\n",
    "]\n",
    "\n",
    "# print header\n",
    "print(\" | \".join(hdr[i].ljust(col_widths[i]) for i in range(len(hdr))))\n",
    "print(\"-+-\".join(\"-\" * w for w in col_widths))\n",
    "\n",
    "# print each data row\n",
    "for row in rows[1:]:\n",
    "    print(\" | \".join(row[i].ljust(col_widths[i]) for i in range(len(hdr))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981b958-1ae8-450b-bcce-faffc9bc2db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoenv",
   "language": "python",
   "name": "nanoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
